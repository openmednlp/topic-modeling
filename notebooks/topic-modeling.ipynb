{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from utils import lemmatized_sentence_corpus, line_review, punct_space\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = '../intermediate'\n",
    "unigram_sentences_filepath = os.path.join(intermediate, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate, 'trigram')\n",
    "trigram_sentences_filepath = os.path.join(intermediate, 'trigram_sentences_all.txt')\n",
    "trigram_dictionary_filepath = os.path.join(intermediate, 'trigram_dict_all_diags.dict')\n",
    "trigram_bow_filepath = os.path.join(intermediate, 'trigram_bow_corpus_all_diags.mm')\n",
    "lda_model_filepath = os.path.join(intermediate, 'lda_model_all_diags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/output.txt', 'r') as f:\n",
    "    data_file = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(infile, outfile):\n",
    "    with codecs.open(outfile, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(infile):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if False:\n",
    "    unigram(data_file, unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it \n",
    "\n",
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "\n",
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(' '.join(unigram_sentence),  '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if 1 == 0:\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "\n",
    "# load\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if 1 == 0:\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')\n",
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if 1 == 0:\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "# load\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = u' '.join(bigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + '\\n')\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_stops = stopwords.words('german')\n",
    "# extend by some custom words\n",
    "de_stops.extend([\"jedoch\",\"sowie\",\"datum\"])\n",
    "# include capitals\n",
    "tmp_lst = []\n",
    "for w in de_stops:\n",
    "    tmp_lst.append(w.title())\n",
    "de_stops.extend(tmp_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if 1 == 0:\n",
    "    # runs 4h\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for parsed_record in nlp.pipe(line_review(data_file), batch_size=10000, n_threads=2):\n",
    "\n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_record if not punct_space(token)]\n",
    "\n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "            # remove any remaining stopwords\n",
    "            # list is from nltk\n",
    "            trigram_review = [term for term in trigram_review if term not in de_stops]\n",
    "\n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if 1 == 0:\n",
    "    trigram_reviews = LineSentence(trigram_sentences_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "\n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)\n",
    "\n",
    "if 1 == 1:\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_sentences_filepath))\n",
    "\n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if 1 == 1:\n",
    "    # workers => sets the parallelism, and should be\n",
    "    # set to your number of physical cores minus one\n",
    "    lda = LdaMulticore(trigram_bow_corpus,\n",
    "                       num_topics=50,\n",
    "                       id2word=trigram_dictionary,\n",
    "                       workers=3)\n",
    "\n",
    "    lda.save(lda_model_filepath)\n",
    "\n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=10):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=20):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "\n",
    "\n",
    "for i in range(1,50):\n",
    "    print(\"Topic: \" + str(i))\n",
    "    explore_topic(topic_number=int(i))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
